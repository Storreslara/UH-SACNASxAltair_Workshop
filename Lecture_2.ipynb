{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly import express as px\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "For this lecture we will be using the customer churn dataset. The dataset contains information about customers of a telecom company. The goal is to predict whether a customer will churn (i.e., stop using the company's services) based on the customer's demographic information, account information, and usage of various services.\n",
    "\n",
    "1. Basic data exploration,\n",
    "2. Data cleaning and Dummy encoding of categorical features.\n",
    "3. Use Weight of Evidence (WoE) to transform categorical features into numeric features that can be used for prediction.\n",
    "4. Use Principal Component Analysis (PCA) to reduce the dimensionality of the dataset.\n",
    "5. Use Logistic Regression and to predict whether a customer will churn.\n",
    "6. Build and deploy a basic neural network using TensorFlow and Keras to predict whether a customer will churn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load data\n",
    "churn_df = pd.read_csv('data/telecom_customer_churn.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Data Exploration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# count number of nulls\n",
    "churn_df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if a feature has type object count the number of unique values\n",
    "for col in churn_df.columns:\n",
    "    if churn_df[col].dtype == 'object':\n",
    "        print(f'{col}: {churn_df[col].nunique()} unique Categories')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Number of unique cities: ',churn_df['City'].nunique())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print all the categories in a feature\n",
    "feat = 'Streaming TV'\n",
    "print(f'Categories in the feature \"{feat}\": ')\n",
    "for reason in churn_df[feat].unique():\n",
    "    print('\\t',reason)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print the 10 cities with the most customers\n",
    "churn_df['City'].value_counts().head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df['Customer Status'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we want to find the customer status for each city, so we group by city and get the value counts of Customer Status\n",
    "churn_df.groupby('City')['Customer Status'].value_counts().head(25)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use px.density_mapbox to show the number of customers in each city, center the map on California\n",
    "fig = px.density_mapbox(churn_df, lat='Latitude', lon='Longitude', radius=10, zoom=5.5, mapbox_style='stamen-terrain', height=1200, width=900)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop irrelevant features\n",
    "garbage_cols = ['Gender','Customer ID','Latitude', 'Longitude', 'Zip Code', 'City',  'Churn Category', 'Churn Reason']\n",
    "churn_df_clean = churn_df.drop(garbage_cols, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df_clean.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we don't care about customers that just Joined, so let's drop them\n",
    "churn_df_clean = churn_df_clean[churn_df_clean['Customer Status'] != 'Joined']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# map the values of the feature 'Customer Status' to numerical values and call it Churned\n",
    "churn_df_clean['Churned'] = churn_df_clean['Customer Status'].map({'Stayed': 0, 'Churned':1})\n",
    "churn_df_clean = churn_df_clean.drop('Customer Status', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dealing with missing values, in our categorical features we will replace the missing values with 'Not Applicable'\n",
    "categorical_features = [col for col in churn_df_clean.columns if churn_df_clean[col].dtype == 'object']\n",
    "print(categorical_features)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# replace missing values with 'Not Applicable'\n",
    "for col in categorical_features:\n",
    "    churn_df_clean[col] = churn_df_clean[col].fillna('Not Applicable')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# you can also replace missing values with the mode of the feature\n",
    "#  in case you forgot mode is the most frequent value in a feature\n",
    "for col in categorical_features:\n",
    "    churn_df_clean[col] = churn_df_clean[col].fillna(churn_df_clean[col].mode()[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df_clean.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# there a few numerical features with missing values, we will replace them with the mean of the feature\n",
    "numerical_features = [col for col in churn_df_clean.columns if churn_df_clean[col].dtype != 'object']\n",
    "print(numerical_features)\n",
    "\n",
    "for num_f in numerical_features:\n",
    "    churn_df_clean[num_f] = churn_df_clean[num_f].fillna(churn_df_clean[num_f].mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df_clean.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dummy encoding is a common way to transform categorical features into numeric features that can be used for prediction. The process involves creating a new binary feature for each category in a categorical feature. For example, the feature \"Offer\" has 5 categories: \"Offer A\", \"Offer B\", \"Offer C\", \"Offer D\", and \"Offer E\". Dummy encoding will create 5 new binary features: \"Offer A\", \"Offer B\", \"Offer C\", \"Offer D\", and \"Offer E\". Each of these new features will have a value of 1 if the customer was offered that particular offer and 0 otherwise.\n",
    "\n",
    "While thi can be useful, features with a large number of categories can lead to a large number of new features. For example, the feature \"City\" has 112 categories. Dummy encoding will create 112 new features, one for each city. This can lead to a large number of features and the curse of dimensionality. To avoid this, we will only dummy encode features with a small number of categories."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dummy encode the categorical features\n",
    "churn_df_dum = pd.get_dummies(churn_df_clean, columns=categorical_features, drop_first=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df_dum.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3.2 Weight of Evidence (WoE) and Information Value (IV)\n",
    "\n",
    "Weight of Evidence (WoE) is a statistical technique commonly used in credit scoring and other applications to assess the predictive power of independent variables in a logistic regression model. It involves transforming categorical variables into numeric values that can be used as inputs for predictive models. Here's how you can use WoE:\n",
    "\n",
    "$$\\text{WoE} = \\ln\\left(\\frac{\\text{Distribution of Good}}{\\text{Distribution of Bad}}\\right)$$\n",
    "\n",
    "Understand the purpose: WoE is used to measure the relationship between a categorical variable and the likelihood of an event occurring (e.g., defaulting on a loan or in our case a customer churn). It calculates the relative \"evidence\" provided by each category in predicting the event.\n",
    "\n",
    "1. *Calculate the WoE*: To compute the WoE for each category of a categorical variable, follow these steps:\n",
    "\n",
    "    a. For each category, calculate the proportion of events (e.g., customer churn) and non-events (e.g., stayed).\n",
    "\n",
    "    b. Calculate the ratio of event proportion to non-event proportion for each category.\n",
    "\n",
    "    c. Take the natural logarithm of the ratio obtained in step b.\n",
    "\n",
    "    d. Multiply the result from step c by 100 to scale the WoE values.\n",
    "\n",
    "    The formula for WoE is: WoE = ln(Event Proportion / Non-event Proportion)\n",
    "\n",
    "2. *Replace categorical values with WoE*: Once you have calculated the WoE for each category, you can replace the original categorical values in your dataset with their corresponding WoE values. This transformation ensures that the categorical variable retains its predictive power while being expressed numerically.\n",
    "\n",
    "3. *Handle missing values*: If you have missing values in your categorical variable, you can assign a separate category or use a special WoE value to represent those missing values.\n",
    "\n",
    "4. *Interpretation*: After converting categorical values to WoE, you can interpret the magnitude and direction of the WoE values. Higher positive values indicate a higher likelihood of the event occurring, while lower negative values indicate a lower likelihood. A value of zero means that the event and non-event proportions are equal.\n",
    "\n",
    "Information Value (IV) is a measure of the predictive power of an independent variable in a logistic regression model. It is calculated by summing the differences between the proportions of events and non-events for each category of the variable, multiplied by the WoE for that category. The higher the IV, the more predictive power the variable has. Here's how you can use IV:\n",
    "$$\\text{IV} = \\sum_{i=1}^{n} (\\text{Distribution of Good}_i - \\text{Distribution of Bad}_i) \\times \\text{WoE}_i$$\n",
    "1. *Calculate the IV*: To compute the IV for each category of a categorical variable, follow these steps:\n",
    "\n",
    "    a. For each category, calculate the proportion of events (e.g., customer churn) and non-events (e.g., stayed).\n",
    "\n",
    "    b. Calculate the difference between the event proportion and non-event proportion for each category.\n",
    "\n",
    "    c. Multiply the result from step b by the WoE for that category.\n",
    "\n",
    "    d. Sum the results from step c to obtain the IV for the variable.\n",
    "\n",
    "    The formula for IV is: IV = sum((Event Proportion - Non-event Proportion) * WoE)\n",
    "2. *Interpretation*: After calculating the IV for each variable, you can interpret the predictive power of each variable. According to the literature, the following guidelines can be used to interpret the IV:\n",
    "\n",
    "    * $< 0.02$: Useless for prediction\n",
    "    * $0.02 \\text{ to } 0.1$: Weak predictor\n",
    "    * $0.1 \\text{ to } 0.3$: Medium predictor\n",
    "    * $0.3 \\text{ to } 0.5$: Strong predictor\n",
    "    * $> 0.5$: Suspicious predictor\n",
    "\n",
    "What makes a variable a suspicious predictor? According to the literature, a variable with an IV greater than 0.5 is too good to be true and may indicate data leakage or other problems. In this case, you should investigate further to determine the cause of the high IV.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a function that calculates the weight of evidence of each category in a feature\n",
    "#  add information value of the feature\n",
    "def calc_weight_of_evidence(df, target, num_bins=10):\n",
    "    # create good and bad columns, by mapping the target feature to 1 and 0\n",
    "    df[f'Good'] = np.where(df[target] == 0, 1, 0)\n",
    "    df[f'Bad'] = np.where(df[target] == 1, 1, 0)\n",
    "    total_good = df['Good'].sum()\n",
    "    total_bad = df['Bad'].sum()\n",
    "    iv = {}\n",
    "    # after cleaning get the list of categorical features\n",
    "    categorical_features = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    for feature in categorical_features:\n",
    "        # ignore the target feature and Good and Bad columns\n",
    "        if feature == target or feature == 'Good' or feature == 'Bad':\n",
    "            continue\n",
    "\n",
    "        # dealing with categorical\n",
    "        if df[feature].dtype == 'object':\n",
    "            # group by each category in the feature and calculate the WoE, binning the feature values\n",
    "            grouped = df.groupby(feature).agg({'Good': 'sum', 'Bad': 'sum'})\n",
    "            # create a DistributionGood and DistributionBad column to calculate the proportion of each category, add 0.5 * len(grouped) to avoid division by zero\n",
    "            grouped['DistributionGood'] = (grouped['Good'] + 0.5) / (total_good + 0.5 * len(grouped))\n",
    "            grouped['DistributionBad'] = (grouped['Bad'] + 0.5) / (total_bad + 0.5 * len(grouped))\n",
    "            # calculate the WoE\n",
    "            grouped['WoE'] = np.log(grouped['DistributionGood'] / grouped['DistributionBad'])\n",
    "            # make a woe dictionary to map each category to its corresponding WoE value\n",
    "            woe_dict = grouped['WoE'].to_dict()\n",
    "            df[feature] = df[feature].map(woe_dict)\n",
    "            # calculate the information value of the feature and add it to the iv dictionary\n",
    "            information_value = ((grouped['DistributionGood'] - grouped['DistributionBad']) * grouped['WoE']).sum()\n",
    "            iv[feature] = information_value\n",
    "        # dealing with numerical\n",
    "        if df[feature].dtype != 'object':\n",
    "            # binning the feature values\n",
    "            df[feature], bins = pd.cut(df[feature], bins=num_bins, retbins=True, labels=False)\n",
    "            # group by each category in the feature and calculate the WoE\n",
    "            grouped = df.groupby(feature).agg({'Good': 'sum', 'Bad': 'sum'})\n",
    "            # create a DistributionGood and DistributionBad column to calculate the proportion of each category, add 0.5 * len(grouped) to avoid division by zero\n",
    "            grouped['DistributionGood'] = (grouped['Good'] + 0.5) / (total_good + 0.5 * len(grouped))\n",
    "            grouped['DistributionBad'] = (grouped['Bad'] + 0.5) / (total_bad + 0.5 * len(grouped))\n",
    "            # calculate the WoE\n",
    "            grouped['WoE'] = np.log(grouped['DistributionGood'] / grouped['DistributionBad'])\n",
    "            # make a woe dictionary to map each category to its corresponding WoE value\n",
    "            woe_dict = grouped['WoE'].to_dict()\n",
    "            df[feature] = df[feature].map(woe_dict)\n",
    "            # calculate the information value of the feature and add it to the iv dictionary\n",
    "            information_value = ((grouped['DistributionGood'] - grouped['DistributionBad']) * grouped['WoE']).sum()\n",
    "            iv[feature] = information_value\n",
    "    df = df.drop(['Good', 'Bad'], axis=1)\n",
    "    return df, iv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate the weight of evidence and IV for the churn df\n",
    "churn_df_woe, churn_iv = calc_weight_of_evidence(churn_df_clean, 'Churned')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df_woe.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print the information value of each feature\n",
    "for feature in churn_iv:\n",
    "    print(feature, churn_iv[feature])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop features with IV < 0.02 and > 0.5\n",
    "bad_features = []\n",
    "for feature in churn_iv:\n",
    "    if churn_iv[feature] <= 0.02 or churn_iv[feature] >= 0.5:\n",
    "        bad_features.append(feature)\n",
    "\n",
    "for feature in bad_features:\n",
    "    print(f'Dropping {feature} because its IV is {churn_iv[feature]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Look at bad features further, let's plot hist of each feature"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for feature in bad_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(churn_df_woe[feature])\n",
    "    plt.title(feature)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df_woe.drop(bad_features, axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df_woe.info()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique that can be used to reduce the number of features in a dataset while retaining most of the information.\n",
    "It does this by creating new features that are combinations of the original features, and then dropping the original features.\n",
    "The new features are known as principal components, and they are orthogonal (i.e., at right angles) to each other.\n",
    "The first principal component captures the largest amount of variation in the data, and each subsequent component captures the largest amount of remaining variation that is orthogonal to the previous components."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use PCA  for churn df\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create a copy of the churn woe df\n",
    "churn_df_pca = churn_df_woe.copy()\n",
    "\n",
    "# drop the target feature\n",
    "churn_df_pca.drop('Churned', axis=1, inplace=True)\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "churn_df_pca = scaler.fit_transform(churn_df_pca)\n",
    "\n",
    "# create a PCA object\n",
    "num_components = 3\n",
    "# these components are the new features\n",
    "\n",
    "pca = PCA(n_components=num_components)\n",
    "\n",
    "# fit the PCA object\n",
    "pca.fit(churn_df_pca)\n",
    "\n",
    "# transform the data\n",
    "churn_df_pca = pca.transform(churn_df_pca)\n",
    "\n",
    "# create a dataframe with the PCA data\n",
    "#  call columns PC_x depending on the component number\n",
    "churn_df_pca = pd.DataFrame(churn_df_pca, columns=[f'PC{x}' for x in range(1, num_components + 1)])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df_pca['Churned'] = churn_df_woe['Churned']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df_woe['Churned'].isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df_pca['Churned'].isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For some reason unknown to me, the PCA is dumping values in the Churned column\n",
    "# So, we need to map the values of Churned in the PCA df back to the original values\n",
    "# get the index for values in Churned that are missing\n",
    "missing_index = churn_df_pca[churn_df_pca['Churned'].isnull()].index\n",
    "# locate the Churned values in the original df that correspond to the missing values in the PCA df\n",
    "churn_df_pca['Churned'].iloc[missing_index] = churn_df_woe['Churned'].iloc[missing_index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df_pca['Churned'].isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# just to make sure\n",
    "churn_df_pca.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the data px scatter plot\n",
    "px.scatter(churn_df_pca, x='PC1', y='PC2', color='Churned')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Logistic Regression\n",
    "\n",
    "Logistic regression is a statistical model that uses a logistic function to model a binary dependent variable.\n",
    "In our case, the dependent variable is whether a customer has churned or not.\n",
    "Logistic regression is a popular technique for modeling customer churn because it is easy to interpret and implement, and it performs well on simple datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use logistic regression for churn_df_pca\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create a copy of the churn df\n",
    "churn_df_lr = churn_df_pca.copy()\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(churn_df_lr.drop('Churned', axis=1), churn_df_lr['Churned'], test_size=0.2, random_state=42)\n",
    "\n",
    "# create a logistic regression object\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# fit the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Model Evaluation\n",
    "\n",
    "**Accuracy** is the proportion of correct predictions out of all predictions made. It is a good measure when the classes are balanced, but it can be misleading when there is a large class imbalance.\n",
    "\n",
    "**Precision** is the proportion of correct positive predictions out of all positive predictions made. It is a good measure when the cost of false positives is high.\n",
    "\n",
    "**Recall** is the proportion of correct positive predictions out of all actual positive instances. It is a good measure when the cost of false negatives is high.\n",
    "\n",
    "**F1 score** is the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of precision and recall. It is a good measure when you want to balance precision and recall, and when there is an uneven class distribution.\n",
    "\n",
    "**Confusion matrix** is a table that shows the number of correct and incorrect predictions made by a model. It is a good way to evaluate the performance of a model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def eval_model(y_test, y_pred):\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    print(f'Precision: {precision_score(y_test, y_pred, zero_division=1.0)}')\n",
    "    print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "    print(f'F1 Score: {f1_score(y_test, y_pred,zero_division=1.0)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_model(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# 6. Neural Networks\n",
    "\"Neural Networks (NNs) are a class of machine learning algorithms that draw inspiration from the structure and function of the human brain. They are widely used for solving complex problems in classification and regression tasks.\n",
    "\n",
    "NNs are composed of interconnected layers of artificial neurons. Each neuron receives inputs from the neurons in the previous layer and applies a set of weights to those inputs, along with a bias term. These weighted inputs are then transformed using an activation function to produce an output.\n",
    "\n",
    "One of the key strengths of NNs lies in their ability to learn complex relationships within the data. This is achieved through a process called training, where the NN adjusts its weights and biases based on a training dataset. The objective is to minimize a predefined loss function that measures the disparity between the predicted outputs and the true labels in the training data.\n",
    "\n",
    "\n",
    "During the training process, NNs use optimization algorithms like gradient descent to update the weights and biases iteratively. The backpropagation algorithm plays a crucial role in this process, as it efficiently calculates the gradients of the loss function with respect to the weights and biases, enabling the network to make adjustments that reduce the error.\n",
    "Choosing an appropriate loss function is critical and depends on the problem at hand. For regression tasks, common loss functions include mean squared error (MSE) and mean absolute error (MAE), while for classification tasks, cross-entropy loss is often used.\n",
    "For this lecture we will be using Tensorflow and Keras to build a neural network model. Tensorflow is an open-source machine learning library developed by Google, and Keras is a high-level API that runs on top of Tensorflow. Keras provides a simple and intuitive interface for building neural networks, while Tensorflow provides the backend for executing the computations required by the network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build a tensorflow model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# create a copy of the churn df\n",
    "churn_df_nn = churn_df_pca.copy()\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(churn_df_nn.drop('Churned', axis=1), churn_df_nn['Churned'], test_size=0.2, random_state=42)\n",
    "\n",
    "# create a tensorflow model\n",
    "# use the shape of the df as the input shape\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(churn_df_nn.shape[1]-1,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model.evaluate(X_test, y_test, verbose=0)\n",
    "#  the output is the loss and accuracy of the model\n",
    "#  for binary crossentropy loss, the lower the better"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = [1.0 if pred > 0.5 else 0.0 for pred in preds]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use the custom eval_model function to evaluate the model\n",
    "eval_model(y_test, preds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Those models sucked !! Let's try training them with woe df instead of pca df\n",
    "\n",
    "First let's create our train and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a copy of the churn df\n",
    "churn_df_lr = churn_df_woe.copy()\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(churn_df_lr.drop('Churned', axis=1), churn_df_lr['Churned'], test_size=0.2, random_state=42)\n",
    "\n",
    "# fit and transform the train set\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# transform the test set\n",
    "scaler_test = StandardScaler()\n",
    "X_test = scaler_test.fit_transform(X_test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create the logistic regression object\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# fit the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# evaluate the model using our custom function\n",
    "eval_model(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# now let's create a new TF model\n",
    "# since the shape of our df has changed, we need to update the input shape\n",
    "\n",
    "\n",
    "\n",
    "# create a tensorflow model\n",
    "# use the shape of the df as the input shape\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(churn_df_lr.shape[1]-1,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model.evaluate(X_test, y_test, verbose=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "# don't forget to convert them to 0s and 1s\n",
    "preds = [1.0 if pred > 0.5 else 0.0 for pred in preds]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use the custom eval_model function to evaluate the model\n",
    "eval_model(y_test, preds)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
