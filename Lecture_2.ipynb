{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-19T14:02:32.540633600Z",
     "start_time": "2023-06-19T14:02:31.682648300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly import express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "For this lecture we will be using the customer churn dataset. The dataset contains information about customers of a telecom company. The goal is to predict whether a customer will churn (i.e., stop using the company's services) based on the customer's demographic information, account information, and usage of various services.\n",
    "\n",
    "1. Basic data exploration,\n",
    "2. Data cleaning.\n",
    "3. Use Weight of Evidence (WoE) to transform categorical features into numeric features that can be used for prediction.\n",
    "4. Use Principal Component Analysis (PCA) to reduce the dimensionality of the dataset.\n",
    "5. Use Logistic Regression and to predict whether a customer will churn.\n",
    "6. Build and deploy a basic neural network using TensorFlow and Keras to predict whether a customer will churn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv('data/telecom_customer_churn.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Data Exploration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "  Customer ID  Gender  Age Married  Number of Dependents          City  \\\n0  0002-ORFBO  Female   37     Yes                     0  Frazier Park   \n1  0003-MKNFE    Male   46      No                     0      Glendale   \n2  0004-TLHLJ    Male   50      No                     0    Costa Mesa   \n3  0011-IGKFF    Male   78     Yes                     0      Martinez   \n4  0013-EXCHZ  Female   75     Yes                     0     Camarillo   \n5  0013-MHZWF  Female   23      No                     3      Midpines   \n6  0013-SMEOE  Female   67     Yes                     0        Lompoc   \n7  0014-BMAQU    Male   52     Yes                     0          Napa   \n8  0015-UOCOJ  Female   68      No                     0   Simi Valley   \n9  0016-QLJIS  Female   43     Yes                     1      Sheridan   \n\n   Zip Code   Latitude   Longitude  Number of Referrals  ...   Payment Method  \\\n0     93225  34.827662 -118.999073                    2  ...      Credit Card   \n1     91206  34.162515 -118.203869                    0  ...      Credit Card   \n2     92627  33.645672 -117.922613                    0  ...  Bank Withdrawal   \n3     94553  38.014457 -122.115432                    1  ...  Bank Withdrawal   \n4     93010  34.227846 -119.079903                    3  ...      Credit Card   \n5     95345  37.581496 -119.972762                    0  ...      Credit Card   \n6     93437  34.757477 -120.550507                    1  ...  Bank Withdrawal   \n7     94558  38.489789 -122.270110                    8  ...      Credit Card   \n8     93063  34.296813 -118.685703                    0  ...  Bank Withdrawal   \n9     95681  38.984756 -121.345074                    3  ...      Credit Card   \n\n  Monthly Charge Total Charges  Total Refunds Total Extra Data Charges  \\\n0          65.60        593.30           0.00                        0   \n1          -4.00        542.40          38.33                       10   \n2          73.90        280.85           0.00                        0   \n3          98.00       1237.85           0.00                        0   \n4          83.90        267.40           0.00                        0   \n5          69.40        571.45           0.00                        0   \n6         109.70       7904.25           0.00                        0   \n7          84.65       5377.80           0.00                       20   \n8          48.20        340.35           0.00                        0   \n9          90.45       5957.90           0.00                        0   \n\n  Total Long Distance Charges Total Revenue  Customer Status   Churn Category  \\\n0                      381.51        974.81           Stayed              NaN   \n1                       96.21        610.28           Stayed              NaN   \n2                      134.60        415.45          Churned       Competitor   \n3                      361.66       1599.51          Churned  Dissatisfaction   \n4                       22.14        289.54          Churned  Dissatisfaction   \n5                      150.93        722.38           Stayed              NaN   \n6                      707.16       8611.41           Stayed              NaN   \n7                      816.48       6214.28           Stayed              NaN   \n8                       73.71        414.06           Stayed              NaN   \n9                     1849.90       7807.80           Stayed              NaN   \n\n                    Churn Reason  \n0                            NaN  \n1                            NaN  \n2  Competitor had better devices  \n3        Product dissatisfaction  \n4            Network reliability  \n5                            NaN  \n6                            NaN  \n7                            NaN  \n8                            NaN  \n9                            NaN  \n\n[10 rows x 38 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Customer ID</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Married</th>\n      <th>Number of Dependents</th>\n      <th>City</th>\n      <th>Zip Code</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>Number of Referrals</th>\n      <th>...</th>\n      <th>Payment Method</th>\n      <th>Monthly Charge</th>\n      <th>Total Charges</th>\n      <th>Total Refunds</th>\n      <th>Total Extra Data Charges</th>\n      <th>Total Long Distance Charges</th>\n      <th>Total Revenue</th>\n      <th>Customer Status</th>\n      <th>Churn Category</th>\n      <th>Churn Reason</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0002-ORFBO</td>\n      <td>Female</td>\n      <td>37</td>\n      <td>Yes</td>\n      <td>0</td>\n      <td>Frazier Park</td>\n      <td>93225</td>\n      <td>34.827662</td>\n      <td>-118.999073</td>\n      <td>2</td>\n      <td>...</td>\n      <td>Credit Card</td>\n      <td>65.60</td>\n      <td>593.30</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>381.51</td>\n      <td>974.81</td>\n      <td>Stayed</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0003-MKNFE</td>\n      <td>Male</td>\n      <td>46</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Glendale</td>\n      <td>91206</td>\n      <td>34.162515</td>\n      <td>-118.203869</td>\n      <td>0</td>\n      <td>...</td>\n      <td>Credit Card</td>\n      <td>-4.00</td>\n      <td>542.40</td>\n      <td>38.33</td>\n      <td>10</td>\n      <td>96.21</td>\n      <td>610.28</td>\n      <td>Stayed</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0004-TLHLJ</td>\n      <td>Male</td>\n      <td>50</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Costa Mesa</td>\n      <td>92627</td>\n      <td>33.645672</td>\n      <td>-117.922613</td>\n      <td>0</td>\n      <td>...</td>\n      <td>Bank Withdrawal</td>\n      <td>73.90</td>\n      <td>280.85</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>134.60</td>\n      <td>415.45</td>\n      <td>Churned</td>\n      <td>Competitor</td>\n      <td>Competitor had better devices</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0011-IGKFF</td>\n      <td>Male</td>\n      <td>78</td>\n      <td>Yes</td>\n      <td>0</td>\n      <td>Martinez</td>\n      <td>94553</td>\n      <td>38.014457</td>\n      <td>-122.115432</td>\n      <td>1</td>\n      <td>...</td>\n      <td>Bank Withdrawal</td>\n      <td>98.00</td>\n      <td>1237.85</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>361.66</td>\n      <td>1599.51</td>\n      <td>Churned</td>\n      <td>Dissatisfaction</td>\n      <td>Product dissatisfaction</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0013-EXCHZ</td>\n      <td>Female</td>\n      <td>75</td>\n      <td>Yes</td>\n      <td>0</td>\n      <td>Camarillo</td>\n      <td>93010</td>\n      <td>34.227846</td>\n      <td>-119.079903</td>\n      <td>3</td>\n      <td>...</td>\n      <td>Credit Card</td>\n      <td>83.90</td>\n      <td>267.40</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>22.14</td>\n      <td>289.54</td>\n      <td>Churned</td>\n      <td>Dissatisfaction</td>\n      <td>Network reliability</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0013-MHZWF</td>\n      <td>Female</td>\n      <td>23</td>\n      <td>No</td>\n      <td>3</td>\n      <td>Midpines</td>\n      <td>95345</td>\n      <td>37.581496</td>\n      <td>-119.972762</td>\n      <td>0</td>\n      <td>...</td>\n      <td>Credit Card</td>\n      <td>69.40</td>\n      <td>571.45</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>150.93</td>\n      <td>722.38</td>\n      <td>Stayed</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0013-SMEOE</td>\n      <td>Female</td>\n      <td>67</td>\n      <td>Yes</td>\n      <td>0</td>\n      <td>Lompoc</td>\n      <td>93437</td>\n      <td>34.757477</td>\n      <td>-120.550507</td>\n      <td>1</td>\n      <td>...</td>\n      <td>Bank Withdrawal</td>\n      <td>109.70</td>\n      <td>7904.25</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>707.16</td>\n      <td>8611.41</td>\n      <td>Stayed</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0014-BMAQU</td>\n      <td>Male</td>\n      <td>52</td>\n      <td>Yes</td>\n      <td>0</td>\n      <td>Napa</td>\n      <td>94558</td>\n      <td>38.489789</td>\n      <td>-122.270110</td>\n      <td>8</td>\n      <td>...</td>\n      <td>Credit Card</td>\n      <td>84.65</td>\n      <td>5377.80</td>\n      <td>0.00</td>\n      <td>20</td>\n      <td>816.48</td>\n      <td>6214.28</td>\n      <td>Stayed</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0015-UOCOJ</td>\n      <td>Female</td>\n      <td>68</td>\n      <td>No</td>\n      <td>0</td>\n      <td>Simi Valley</td>\n      <td>93063</td>\n      <td>34.296813</td>\n      <td>-118.685703</td>\n      <td>0</td>\n      <td>...</td>\n      <td>Bank Withdrawal</td>\n      <td>48.20</td>\n      <td>340.35</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>73.71</td>\n      <td>414.06</td>\n      <td>Stayed</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0016-QLJIS</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>Yes</td>\n      <td>1</td>\n      <td>Sheridan</td>\n      <td>95681</td>\n      <td>38.984756</td>\n      <td>-121.345074</td>\n      <td>3</td>\n      <td>...</td>\n      <td>Credit Card</td>\n      <td>90.45</td>\n      <td>5957.90</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>1849.90</td>\n      <td>7807.80</td>\n      <td>Stayed</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows Ã— 38 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 38 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   Customer ID                        7043 non-null   object \n",
      " 1   Gender                             7043 non-null   object \n",
      " 2   Age                                7043 non-null   int64  \n",
      " 3   Married                            7043 non-null   object \n",
      " 4   Number of Dependents               7043 non-null   int64  \n",
      " 5   City                               7043 non-null   object \n",
      " 6   Zip Code                           7043 non-null   int64  \n",
      " 7   Latitude                           7043 non-null   float64\n",
      " 8   Longitude                          7043 non-null   float64\n",
      " 9   Number of Referrals                7043 non-null   int64  \n",
      " 10  Tenure in Months                   7043 non-null   int64  \n",
      " 11  Offer                              7043 non-null   object \n",
      " 12  Phone Service                      7043 non-null   object \n",
      " 13  Avg Monthly Long Distance Charges  6361 non-null   float64\n",
      " 14  Multiple Lines                     6361 non-null   object \n",
      " 15  Internet Service                   7043 non-null   object \n",
      " 16  Internet Type                      5517 non-null   object \n",
      " 17  Avg Monthly GB Download            5517 non-null   float64\n",
      " 18  Online Security                    5517 non-null   object \n",
      " 19  Online Backup                      5517 non-null   object \n",
      " 20  Device Protection Plan             5517 non-null   object \n",
      " 21  Premium Tech Support               5517 non-null   object \n",
      " 22  Streaming TV                       5517 non-null   object \n",
      " 23  Streaming Movies                   5517 non-null   object \n",
      " 24  Streaming Music                    5517 non-null   object \n",
      " 25  Unlimited Data                     5517 non-null   object \n",
      " 26  Contract                           7043 non-null   object \n",
      " 27  Paperless Billing                  7043 non-null   object \n",
      " 28  Payment Method                     7043 non-null   object \n",
      " 29  Monthly Charge                     7043 non-null   float64\n",
      " 30  Total Charges                      7043 non-null   float64\n",
      " 31  Total Refunds                      7043 non-null   float64\n",
      " 32  Total Extra Data Charges           7043 non-null   int64  \n",
      " 33  Total Long Distance Charges        7043 non-null   float64\n",
      " 34  Total Revenue                      7043 non-null   float64\n",
      " 35  Customer Status                    7043 non-null   object \n",
      " 36  Churn Category                     1869 non-null   object \n",
      " 37  Churn Reason                       1869 non-null   object \n",
      "dtypes: float64(9), int64(6), object(23)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "churn_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "Customer ID                             0\nGender                                  0\nAge                                     0\nMarried                                 0\nNumber of Dependents                    0\nCity                                    0\nZip Code                                0\nLatitude                                0\nLongitude                               0\nNumber of Referrals                     0\nTenure in Months                        0\nOffer                                   0\nPhone Service                           0\nAvg Monthly Long Distance Charges     682\nMultiple Lines                        682\nInternet Service                        0\nInternet Type                        1526\nAvg Monthly GB Download              1526\nOnline Security                      1526\nOnline Backup                        1526\nDevice Protection Plan               1526\nPremium Tech Support                 1526\nStreaming TV                         1526\nStreaming Movies                     1526\nStreaming Music                      1526\nUnlimited Data                       1526\nContract                                0\nPaperless Billing                       0\nPayment Method                          0\nMonthly Charge                          0\nTotal Charges                           0\nTotal Refunds                           0\nTotal Extra Data Charges                0\nTotal Long Distance Charges             0\nTotal Revenue                           0\nCustomer Status                         0\nChurn Category                       5174\nChurn Reason                         5174\ndtype: int64"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T15:17:10.937809700Z",
     "start_time": "2023-06-19T15:17:10.903804500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "Let's explore the data to understand the distribution of each feature, and the relationship between different features and the target feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique cities:  1106\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique cities: ',churn_df['City'].nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T15:22:11.245761200Z",
     "start_time": "2023-06-19T15:22:11.228763500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Los Angeles      293\nSan Diego        285\nSan Jose         112\nSacramento       108\nSan Francisco    104\nFresno            61\nLong Beach        60\nOakland           52\nEscondido         51\nStockton          44\nName: City, dtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the 5 cities with the most customers\n",
    "churn_df['City'].value_counts().head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T19:10:10.662078Z",
     "start_time": "2023-06-17T19:10:10.656358Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "Stayed     4720\nChurned    1869\nJoined      454\nName: Customer Status, dtype: int64"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df['Customer Status'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T15:16:26.401916700Z",
     "start_time": "2023-06-19T15:16:26.377902800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# map the values of the feature 'Customer Status' to numerical values\n",
    "churn_df['Customer Status'] = churn_df['Customer Status'].map({'Stayed': 1, 'Churned': 0})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-17T19:10:11.761801Z",
     "start_time": "2023-06-17T19:10:11.756643Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# in churn df find all the features that are categorical\n",
    "categorical_features = [col for col in churn_df.columns if churn_df[col].dtype == 'object']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use px.density_mapbox to show the number of customers in each city, center the map on California\n",
    "fig = px.density_mapbox(churn_df, lat='Latitude', lon='Longitude', radius=10, zoom=5.5, mapbox_style='stamen-terrain', height=1200, width=900)\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Let's drop some features that are not useful for prediction. We'll also drop features that have too many missing values or too many unique values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# drop irrelevant features\n",
    "garbage_cols = ['Gender','Customer ID','Latitude', 'Longitude', 'Zip Code', 'City',  'Churn Category', 'Churn Reason']\n",
    "churn_df_clean = churn_df.drop(garbage_cols, axis=1).copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T15:05:28.568558400Z",
     "start_time": "2023-06-19T15:05:28.538560800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Weight of Evidence (WoE)\n",
    "\n",
    "Weight of Evidence (WoE) is a statistical technique commonly used in credit scoring and other applications to assess the predictive power of independent variables in a logistic regression model. It involves transforming categorical variables into numeric values that can be used as inputs for predictive models. Here's how you can use WoE:\n",
    "\n",
    "Understand the purpose: WoE is used to measure the relationship between a categorical variable and the likelihood of an event occurring (e.g., defaulting on a loan). It calculates the relative \"evidence\" provided by each category in predicting the event.\n",
    "\n",
    "1. *Calculate the WoE*: To compute the WoE for each category of a categorical variable, follow these steps:\n",
    "\n",
    "    a. For each category, calculate the proportion of events (e.g., defaulted loans) and non-events (e.g., non-defaulted loans).\n",
    "\n",
    "    b. Calculate the ratio of event proportion to non-event proportion for each category.\n",
    "\n",
    "    c. Take the natural logarithm of the ratio obtained in step b.\n",
    "\n",
    "    d. Multiply the result from step c by 100 to scale the WoE values.\n",
    "\n",
    "    The formula for WoE is: WoE = ln(Event Proportion / Non-event Proportion) * 100\n",
    "\n",
    "2. *Replace categorical values with WoE*: Once you have calculated the WoE for each category, you can replace the original categorical values in your dataset with their corresponding WoE values. This transformation ensures that the categorical variable retains its predictive power while being expressed numerically.\n",
    "\n",
    "3. *Handle missing values*: If you have missing values in your categorical variable, you can assign a separate category or use a special WoE value to represent those missing values.\n",
    "\n",
    "4. *Interpretation*: After converting categorical values to WoE, you can interpret the magnitude and direction of the WoE values. Higher positive values indicate a higher likelihood of the event occurring, while lower negative values indicate a lower likelihood. A value of zero means that the event and non-event proportions are equal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# create a function that calculates the weight of evidence of each category in a feature\n",
    "#  add information value of the feature\n",
    "def calc_weight_of_evidence(df, target):\n",
    "    df[f'Good'] = np.where(df[target] == 0, 1, 0)\n",
    "    df[f'Bad'] = np.where(df[target] == 1, 1, 0)\n",
    "    total_good = df['Good'].sum()\n",
    "    total_bad = df['Bad'].sum()\n",
    "    iv = {}\n",
    "    for feature in df.columns:\n",
    "        # ignore the target feature\n",
    "        if feature == target or feature == 'Good' or feature == 'Bad':\n",
    "            continue\n",
    "        grouped = df.groupby(feature).agg({'Good': 'sum', 'Bad': 'sum'})\n",
    "        grouped['DistributionGood'] = grouped['Good'] / total_good\n",
    "        grouped['DistributionBad'] = grouped['Bad'] / total_bad\n",
    "        grouped['WoE'] = np.log(grouped['DistributionGood'] / grouped['DistributionBad'])\n",
    "        woe_dict = grouped['WoE'].to_dict()\n",
    "        df[feature] = df[feature].map(woe_dict)\n",
    "        information_value = ((grouped['DistributionGood'] - grouped['DistributionBad']) * grouped['WoE']).sum()\n",
    "        iv[feature] = information_value\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T15:06:46.576818900Z",
     "start_time": "2023-06-19T15:06:46.557818600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Customer Status'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[1;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m churn_df_post_iv \u001B[38;5;241m=\u001B[39m \u001B[43mcalc_weight_of_evidence\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchurn_df_clean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mCustomer Status\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [24]\u001B[0m, in \u001B[0;36mcalc_weight_of_evidence\u001B[1;34m(df, target)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m#  if iv is less than 0.02, then the feature is not useful for prediction, drop it\u001B[39;00m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns:\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43miv\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfeature\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.02\u001B[39m:\n\u001B[0;32m     24\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDropping \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfeature\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with IV \u001B[39m\u001B[38;5;132;01m{\u001B[39;00miv[feature]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     25\u001B[0m         df\u001B[38;5;241m.\u001B[39mdrop(feature, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'Customer Status'"
     ]
    }
   ],
   "source": [
    "# calculate the weight of evidence of each category in each feature\n",
    "churn_df_post_iv = calc_weight_of_evidence(churn_df_clean, 'Customer Status')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-19T15:07:55.980402400Z",
     "start_time": "2023-06-19T15:07:55.967403100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "churn_df_post_iv.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a dimensionality reduction technique that can be used to reduce the number of features in a dataset while retaining most of the information.\n",
    "It does this by creating new features that are combinations of the original features, and then dropping the original features.\n",
    "The new features are known as principal components, and they are orthogonal (i.e., at right angles) to each other.\n",
    "The first principal component captures the largest amount of variation in the data, and each subsequent component captures the largest amount of remaining variation that is orthogonal to the previous components."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use PCA  for churn df\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create a copy of the churn df\n",
    "churn_df_pca = churn_df.copy()\n",
    "\n",
    "# drop the target feature\n",
    "churn_df_pca.drop('Customer Status', axis=1, inplace=True)\n",
    "\n",
    "# drop the categorical features\n",
    "churn_df_pca.drop(categorical_features, axis=1, inplace=True)\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "churn_df_pca = scaler.fit_transform(churn_df_pca)\n",
    "\n",
    "# create a PCA object\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# fit the PCA object\n",
    "pca.fit(churn_df_pca)\n",
    "\n",
    "# transform the data\n",
    "churn_df_pca = pca.transform(churn_df_pca)\n",
    "\n",
    "# create a dataframe with the PCA data\n",
    "churn_df_pca = pd.DataFrame(churn_df_pca, columns=['PC1', 'PC2'])\n",
    "\n",
    "# add the target feature to the dataframe\n",
    "churn_df_pca['Customer Status'] = churn_df['Customer Status']\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Logistic Regression\n",
    "\n",
    "Logistic regression is a statistical model that uses a logistic function to model a binary dependent variable.\n",
    "In our case, the dependent variable is whether a customer has churned or not.\n",
    "Logistic regression is a popular technique for modeling customer churn because it is easy to interpret and implement, and it performs well on simple datasets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use logistic regression for churn_df_pca\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create a copy of the churn df\n",
    "churn_df_lr = churn_df_pca.copy()\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(churn_df_lr.drop('Customer Status', axis=1), churn_df_lr['Customer Status'], test_size=0.2, random_state=42)\n",
    "\n",
    "# create a logistic regression object\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# fit the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Model Evaluation\n",
    "\n",
    "**Accuracy** is the proportion of correct predictions out of all predictions made. It is a good measure when the classes are balanced, but it can be misleading when there is a large class imbalance.\n",
    "\n",
    "**Precision** is the proportion of correct positive predictions out of all positive predictions made. It is a good measure when the cost of false positives is high.\n",
    "\n",
    "**Recall** is the proportion of correct positive predictions out of all actual positive instances. It is a good measure when the cost of false negatives is high.\n",
    "\n",
    "**F1 score** is the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of precision and recall. It is a good measure when you want to balance precision and recall, and when there is an uneven class distribution.\n",
    "\n",
    "**Confusion matrix** is a table that shows the number of correct and incorrect predictions made by a model. It is a good way to evaluate the performance of a model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def eval_model(y_test, y_pred):\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "    print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "    print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "    print(f'F1 Score: {f1_score(y_test, y_pred)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eval_model(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Neural Networks\n",
    "\n",
    "\"Neural Networks (NNs) are a class of machine learning algorithms that draw inspiration from the structure and function of the human brain. They are widely used for solving complex problems in classification and regression tasks.\n",
    "\n",
    "NNs are composed of interconnected layers of artificial neurons. Each neuron receives inputs from the neurons in the previous layer and applies a set of weights to those inputs, along with a bias term. These weighted inputs are then transformed using an activation function to produce an output.\n",
    "\n",
    "One of the key strengths of NNs lies in their ability to learn complex relationships within the data. This is achieved through a process called training, where the NN adjusts its weights and biases based on a training dataset. The objective is to minimize a predefined loss function that measures the disparity between the predicted outputs and the true labels in the training data.\n",
    "\n",
    "During the training process, NNs use optimization algorithms like gradient descent to update the weights and biases iteratively. The backpropagation algorithm plays a crucial role in this process, as it efficiently calculates the gradients of the loss function with respect to the weights and biases, enabling the network to make adjustments that reduce the error.\n",
    "\n",
    "Choosing an appropriate loss function is critical and depends on the problem at hand. For regression tasks, common loss functions include mean squared error (MSE) and mean absolute error (MAE), while for classification tasks, cross-entropy loss is often used.\n",
    "\n",
    "For this lecture we will be using Tensorflow and Keras to build a neural network model. Tensorflow is an open-source machine learning library developed by Google, and Keras is a high-level API that runs on top of Tensorflow. Keras provides a simple and intuitive interface for building neural networks, while Tensorflow provides the backend for executing the computations required by the network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# in case you haven't installed tensorflow run this !\n",
    "!pip install tensorflow"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build a tensorflow model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# create a copy of the churn df\n",
    "churn_df_nn = churn_df_pca.copy()\n",
    "\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(churn_df_nn.drop('Customer Status', axis=1), churn_df_nn['Customer Status'], test_size=0.2, random_state=42)\n",
    "\n",
    "# create a tensorflow model\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=(2,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "model.evaluate(X_test, y_test, verbose=0)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use the custom eval_model function to evaluate the model\n",
    "eval_model(y_test, model.predict_classes(X_test).reshape(-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
